{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bDF6azsrNdte"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS SECTION"
      ],
      "metadata": {
        "id": "MOn-K2yT_TyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.utils\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import albumentations as A\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "K3DMYft2_Qr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchSummary\n",
        "!pip install albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bNXGX6kAHZq",
        "outputId": "eec6bbf3-4d0d-4dd8-adcd-b360f12b6b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchSummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.8.1.78)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER SECTIONS"
      ],
      "metadata": {
        "id": "J59h_N9O_4Qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount DRIVE and FOLDERS"
      ],
      "metadata": {
        "id": "15aUcl8GA9BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-VpBxpE_BBh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d3e452-636a-45a7-83d2-20b899e103cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Garbage classification"
      ],
      "metadata": {
        "id": "t7T1My4WBD-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda6cacb-9218-4537-cb5a-702f068b5908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Garbage classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Garbage classification\"\n",
        "os.listdir(path)\n",
        "glob.glob(path+\"/*\")"
      ],
      "metadata": {
        "id": "0HzumNCCBFdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db019f40-ef86-4333-a20e-3261167e56fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Garbage classification/cardboard',\n",
              " '/content/drive/MyDrive/Garbage classification/trash',\n",
              " '/content/drive/MyDrive/Garbage classification/metal',\n",
              " '/content/drive/MyDrive/Garbage classification/glass',\n",
              " '/content/drive/MyDrive/Garbage classification/paper',\n",
              " '/content/drive/MyDrive/Garbage classification/plastic']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folders = glob.glob(path+\"/*\")\n",
        "count = 0\n",
        "\n",
        "for i in folders:\n",
        "    count=0\n",
        "    count+=len(os.listdir(i))\n",
        "    print(i,' =  ', count)"
      ],
      "metadata": {
        "id": "1PHp9Vq8BIPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6ffb49-9ad7-41c3-f547-66ed362b0aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Garbage classification/cardboard  =   403\n",
            "/content/drive/MyDrive/Garbage classification/trash  =   137\n",
            "/content/drive/MyDrive/Garbage classification/metal  =   410\n",
            "/content/drive/MyDrive/Garbage classification/glass  =   501\n",
            "/content/drive/MyDrive/Garbage classification/paper  =   594\n",
            "/content/drive/MyDrive/Garbage classification/plastic  =   492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Transformation Used"
      ],
      "metadata": {
        "id": "LOGD3YZjAkuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform2  = A.Compose([\n",
        "    A.HorizontalFlip(p=0.2),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2),\n",
        "    A.VerticalFlip(p=0.2),\n",
        "    A.Rotate(0,80),\n",
        "    A.Resize(224,224),\n",
        "    A.Normalize(max_pixel_value = 255.0,p=1)\n",
        "])"
      ],
      "metadata": {
        "id": "tXFQgtig_70o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom Dataset Object"
      ],
      "metadata": {
        "id": "jso83Rm9Byly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import imageio as iio\n",
        "import PIL as image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, transform = None):\n",
        "        self.folder = path\n",
        "        folders = glob.glob(self.folder + \"/*\")\n",
        "#         print('folders',folders)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        for class_path in folders:\n",
        "            class_name = class_path.split(\"/\")[-1]\n",
        "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
        "                self.data.append([img_path, class_name])\n",
        "\n",
        "#         print(self.data[0])\n",
        "\n",
        "\n",
        "        self.class_map = {\n",
        "            'metal' : 0,\n",
        "            'glass' : 1,\n",
        "            'plastic' : 2,\n",
        "            'paper':3,\n",
        "            'cardboard':4,\n",
        "            'trash' :5\n",
        "\n",
        "        }\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.data[idx]\n",
        "\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (224,224))\n",
        "\n",
        "        # Augmentations\n",
        "        if self.transform is not None:\n",
        "            transformed = transform2(image = img)\n",
        "            transformed_image = transformed['image']\n",
        "            img = cv2.resize(img,(224,224))\n",
        "\n",
        "\n",
        "        class_id = self.class_map[class_name]\n",
        "        img_tensor = torch.from_numpy(img)\n",
        "        img_tensor = img_tensor.permute(2, 0, 1)\n",
        "        img_tensor = img_tensor.to(torch.float32)\n",
        "\n",
        "        class_id = class_id\n",
        "        return img_tensor, class_id"
      ],
      "metadata": {
        "id": "sKJiuhFSAw1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(transform = transform2)\n",
        "len(dataset) # 2527 images"
      ],
      "metadata": {
        "id": "f2OPStD1BMFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f084d7-537f-4cd0-e5a6-943d6fc0179d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2537"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split Dataset to Test, Train, Validation with Dataloaders"
      ],
      "metadata": {
        "id": "kx4RoFciB4mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(dataset)\n",
        "train_size = int(0.7 * dataset_size)  # 70% for training\n",
        "val_size = int(0.15 * dataset_size)  # 15% for validation\n",
        "test_size = dataset_size - train_size - val_size  # Remaining for test\n",
        "\n",
        "# Use random_split to split the dataset\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])"
      ],
      "metadata": {
        "id": "V3zxtLQCBN3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "R0dDXYlPCFT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NETWORK DEFINITION"
      ],
      "metadata": {
        "id": "wg8gl-JaCPg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Primary CNN Classifier"
      ],
      "metadata": {
        "id": "HmNvmcf-H3wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"a1.txt\", \"w\")"
      ],
      "metadata": {
        "id": "-xbiDPFcg8Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "num_ftrs = resnet.fc.in_features\n",
        "\n",
        "resnet.fc = nn.Linear(num_ftrs, 3)\n",
        "\n",
        "class MPP_CGT_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MPP_CGT_Classifier, self).__init__()\n",
        "        # Define layers for MPP vs CGT classification\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)  # Two classes: MPP and CGT\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        # Modify the output layer to match the number of classes in your subcategories (MPP or CGT)\n",
        "        self.resnet.fc = nn.Linear(num_ftrs, 3)  # Change 'num_classes' accordingly\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass logic\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 56 * 56)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mpp_cgt_output = self.fc2(x)\n",
        "        probs = F.softmax(mpp_cgt_output, dim=1)\n",
        "        # Predict the class index based on probabilities\n",
        "        # _, predicted_mpp_cgt = torch.max(input = probs, dim = 1)\n",
        "        _, predicted_mpp_cgt = torch.max(mpp_cgt_output, 1)\n",
        "        # print(_, file = f)\n",
        "\n",
        "        if _[0] > _[1]:\n",
        "          predicted_mpp_cgt = 0\n",
        "        else:\n",
        "          predicted_mpp_cgt = 1\n",
        "\n",
        "        if predicted_mpp_cgt == 0:  # MPP category\n",
        "          # Pass through MPP ResNet-18\n",
        "          print(\"BEFORE, RESNET\", file = f)\n",
        "          # x = x.unsqueeze(0)  # Assuming you have a single image, adds a batch dimension\n",
        "          # Resize the tensor using interpolation\n",
        "          x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)  # Adjust size as needed\n",
        "          resnet_output = self.resnet(x)\n",
        "          print(\"AFTER, RESNET\", file = f)\n",
        "          return resnet_output  # Return the output from ResNet-18 for MPP category\n",
        "\n",
        "        elif predicted_mpp_cgt == 1:  # CGT category\n",
        "          # Pass through CGT ResNet-18\n",
        "          print(\"BEFORE, RESNET\", file = f)\n",
        "          # x = x.unsqueeze(0)  # Assuming you have a single image, adds a batch dimension\n",
        "          # Resize the tensor using interpolation\n",
        "          x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)  # Adjust size as needed\n",
        "          resnet_output = self.resnet(x)\n",
        "          print(\"AFTER, RESNET\", file = f)\n",
        "          return resnet_output  # Return the output from ResNet-18 for CGT category"
      ],
      "metadata": {
        "id": "2rv9S2J7CSO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpp_cgt_classifier = MPP_CGT_Classifier()"
      ],
      "metadata": {
        "id": "oi0fxWigIoAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "mpp_cgt_classifier.to(device)\n",
        "summary(mpp_cgt_classifier, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "nj303u1wXK8J",
        "outputId": "21d89082-706c-4578-e6a7-8b4f570686e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-b36e1a7deb13>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmpp_cgt_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmpp_cgt_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-889e0bb61145>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0;31m# x = x.unsqueeze(0)  # Assuming you have a single image, adds a batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0;31m# Resize the tensor using interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust size as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m           \u001b[0mresnet_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AFTER, RESNET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3915\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3916\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   3917\u001b[0m                     \u001b[0;34m\"Input and output must have the same number of spatial dimensions, but got \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3918\u001b[0m                     \u001b[0;34mf\"input with spatial dimensions of {list(input.shape[2:])} and output size of {size}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [] and output size of (224, 224). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"a1.txt\", \"r\")\n",
        "print(f.read())"
      ],
      "metadata": {
        "id": "P8zDRiUDhpN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criterion and Optimizers for the Respective Models"
      ],
      "metadata": {
        "id": "mZFZdJ4aH783"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "MPP_CGT_OPTIMIZER = optim.Adam(mpp_cgt_classifier.parameters(), lr = 0.001)\n",
        "RESNET_MPP_OPTIMIZER = optim.Adam(resnet.parameters(), lr = 0.001)\n",
        "RESNET_CGT_OPTIMIZER = optim.Adam(resnet.parameters(), lr = 0.001)\n",
        "optimizer = optim.Adam(mpp_cgt_classifier.parameters(), lr = 0.001) # torch.optim.Adam(list(resnet.parameters()) + list(mpp_cgt_classifier.parameters()), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True)"
      ],
      "metadata": {
        "id": "eNK64lKMIMfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "k9feeUH5NF6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating Model for Validation"
      ],
      "metadata": {
        "id": "bDF6azsrNdte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()  # for batch normalization layers\n",
        "    corrects = 0\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            corrects += (preds == targets.data).sum()\n",
        "\n",
        "\n",
        "\n",
        "            output = (torch.max(torch.exp(outputs), 1)[1]).data.cpu().numpy()\n",
        "            y_pred.extend(output) # Save Prediction\n",
        "\n",
        "            labels = targets.data.cpu().numpy()\n",
        "            y_true.extend(labels) # Save Truth\n",
        "\n",
        "\n",
        "    print('Validation Accuracy: {:.2f}'.format(100. * corrects / len(dataloader.dataset)))\n",
        "    return y_pred, y_true"
      ],
      "metadata": {
        "id": "x-JRQdUFN0ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Details"
      ],
      "metadata": {
        "id": "xe3eb4S5NMKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "val_losses = []\n",
        "train_losses = []"
      ],
      "metadata": {
        "id": "XPO_dFaDNHiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "t0 = time.time()\n",
        "resnet_num_epochs = 50\n",
        "resnet_val_losses = []\n",
        "resnet_train_losses = []\n",
        "\n",
        "for epoch in range(resnet_num_epochs):\n",
        "    train_loss= 0.0\n",
        "    for i, (inputs,targets) in enumerate(train_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = mpp_cgt_classifier(inputs)\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = criterion(outputs, targets)\n",
        "            #         print(train_loss.item())\n",
        "            # backward pass\n",
        "        train_loss.backward()\n",
        "            # update parameters\n",
        "        optimizer.step()\n",
        "    resnet_train_losses.append(train_loss.item())\n",
        "\n",
        "    mpp_cgt_classifier.eval()\n",
        "    val_loss = 0.0\n",
        "    for i, (inputs, targets) in enumerate(val_dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "            # forward pass\n",
        "        outputs = mpp_cgt_classifier(inputs)\n",
        "\n",
        "        val_loss = criterion(outputs, targets)\n",
        "\n",
        "    resnet_val_losses.append(val_loss.item())\n",
        "\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print('training Loss at epoch ', epoch+1 , \"=\" ,train_loss.item())\n",
        "    print('validation Loss at epoch ', epoch+1 , \"=\" ,val_loss.item())\n",
        "    evaluate_model(mpp_cgt_classifier, val_dataloader, device)\n",
        "t1 = time.time()\n",
        "print(\"Total time taken \" ,(t1-t0)/60 )"
      ],
      "metadata": {
        "id": "nppeNOP0OtdV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}